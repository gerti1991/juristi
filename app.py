"""
Enhanced Albanian Legal RAG System with Cloud LLM Integration
Now uses Groq API for advanced response generation with environment variables
"""

import os
import streamlit as st
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import json
from datetime import datetime
from scraper import AlbanianLegalScraper
from ai import CloudLLMClient
from dotenv import load_dotenv
import google.generativeai as genai
from typing import List, Dict, Optional

# Load environment variables
load_dotenv()

# Detect headless mode (FastAPI)
HEADLESS = os.getenv("RAG_HEADLESS", "0") == "1"

# Set environment variables for PyTorch compatibility
os.environ['TORCH_SHOW_CPP_STACKTRACES'] = '0'
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

class CloudEnhancedAlbanianLegalRAG:
    def __init__(self):
        """Initialize the cloud-enhanced RAG system with Google embeddings"""
        # Embedding configuration
        self.use_google_embeddings = True
        self.model_name = 'all-MiniLM-L6-v2'  # Fallback model
        self.model = None
        self.gemini_api_key = None

        # Document storage
        self.legal_documents = []
        self.document_embeddings = None
        # Super-chunking config (character-based)
        self.superchunk_chars = int(os.getenv('RAG_SUPERCHUNK_CHARS', '4500'))
        self.superchunk_overlap = int(os.getenv('RAG_SUPERCHUNK_OVERLAP', '500'))
        # Sparse retrieval
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.scraper = AlbanianLegalScraper(max_docs=10)
        self.cloud_llm = None

        # Configuration from environment variables
        self.similarity_threshold = float(os.getenv('SIMILARITY_THRESHOLD', '0.15'))
        self.max_context_length = int(os.getenv('MAX_CONTEXT_LENGTH', '4000'))  # characters for packing
        self.max_chunks_to_return = int(os.getenv('MAX_CHUNKS_TO_RETURN', '5'))
        # Hybrid + reranking
        self.hybrid_alpha = float(os.getenv('RAG_HYBRID_ALPHA', '0.5'))
        self.mmr_lambda = float(os.getenv('RAG_MMR_LAMBDA', '0.5'))
        self.neighbor_expansion = int(os.getenv('RAG_NEIGHBOR_EXPANSION', '1'))
        # Conversation memory (in-process)
        self.memory_store = {}
        # Context packing (extractive) to reduce token usage
        self.context_packing = os.getenv('RAG_CONTEXT_PACKING', '1') == '1'

        # Cache file for embeddings
        self.embeddings_cache_file = 'document_embeddings_cache_google.json'

        # Initialize components
        self._setup_google_api()
        self._load_fallback_model()
        self._load_cloud_llm()
        self._load_all_documents()
        self._generate_embeddings()

    # -----------------------------
    # Index building helpers
    # -----------------------------
    def _build_sparse_index(self, document_texts: List[str]):
        """Build TF-IDF index for sparse retrieval."""
        try:
            self.tfidf_vectorizer = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(document_texts)
            if not HEADLESS:
                st.success("âœ… Built TFâ€‘IDF index for hybrid retrieval")
        except Exception as e:
            if not HEADLESS:
                st.warning(f"âš ï¸ Could not build TFâ€‘IDF index: {e}")
            self.tfidf_vectorizer = None
            self.tfidf_matrix = None
    
    def _setup_google_api(self):
        """Setup Google API for embeddings"""
        try:
            self.gemini_api_key = os.getenv("GEMINI_API_KEY")
            if self.gemini_api_key:
                genai.configure(api_key=self.gemini_api_key)
                if not HEADLESS:
                    st.info("ğŸ”„ Google API configured for embeddings...")
                self.use_google_embeddings = True
                if not HEADLESS:
                    st.success("âœ… Google embeddings enabled!")
            else:
                if not HEADLESS:
                    st.warning("âš ï¸ GEMINI_API_KEY not found - using local embeddings")
                self.use_google_embeddings = False
        except Exception as e:
            if not HEADLESS:
                st.error(f"âŒ Error setting up Google API: {e}")
            self.use_google_embeddings = False
    
    def _load_fallback_model(self):
        """Load the local sentence transformer model as fallback"""
        if not self.use_google_embeddings:
            try:
                # Lazy import to avoid torch dependency unless needed
                from sentence_transformers import SentenceTransformer  # type: ignore
                if not HEADLESS:
                    st.info("ğŸ”„ Loading local AI model...")
                self.model = SentenceTransformer(self.model_name)
                if not HEADLESS:
                    st.success("âœ… Local AI model loaded successfully!")
            except Exception as e:
                if not HEADLESS:
                    st.error(f"âŒ Error loading local model: {e}")
                    st.info("Local embeddings disabled. Using Google embeddings only.")
                self.model = None

    def _get_google_embeddings(self, texts: List[str]) -> Optional[np.ndarray]:
        """Get embeddings using Google's embedding-001 model"""
        if not self.use_google_embeddings:
            return None
            
        try:
            # Use Google's text-embedding-004 model (latest) â€” call per text
            embeddings: List[List[float]] = []
            for text in texts:
                res = genai.embed_content(
                    model="models/text-embedding-004",
                    content=text,
                    task_type="retrieval_document"
                )
                vec = res.get('embedding') if isinstance(res, dict) else getattr(res, 'embedding', None)
                if not vec:
                    raise ValueError("Empty embedding from Google API")
                embeddings.append(vec)

            return np.array(embeddings, dtype=float)
            
        except Exception as e:
            if not HEADLESS:
                st.error(f"âŒ Google embeddings error: {e}")
                st.info("Falling back to local embeddings...")
            self.use_google_embeddings = False
            return None

    def _get_google_query_embedding(self, query: str) -> Optional[np.ndarray]:
        """Get embedding for a query using Google's model"""
        if not self.use_google_embeddings:
            return None
            
        try:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=query,
                task_type="retrieval_query"  # Optimized for query embedding
            )
            
            vec = result.get('embedding') if isinstance(result, dict) else getattr(result, 'embedding', None)
            if not vec:
                return None
            return np.array([vec], dtype=float)
            
        except Exception as e:
            if not HEADLESS:
                st.error(f"âŒ Google query embedding error: {e}")
            return None
    
    def _load_cloud_llm(self):
        """Initialize cloud LLM client"""
        try:
            if not HEADLESS:
                st.info("ğŸŒ Connecting to cloud LLM...")
            self.cloud_llm = CloudLLMClient("gemini")  # Changed to Gemini as default
            
            if self.cloud_llm.api_key:
                if not HEADLESS:
                    st.success("âœ… Cloud LLM connected successfully!")
                    st.info(f"ğŸ¤– Using model: {self.cloud_llm.model}")
            else:
                if not HEADLESS:
                    st.warning("âš ï¸ Cloud LLM API key not found - using fallback responses")
        except Exception as e:
            if not HEADLESS:
                st.error(f"âŒ Cloud LLM connection error: {e}")
                st.info("Will use template-based responses as fallback")
            self.cloud_llm = None
    
    def _load_hardcoded_documents(self):
        """Load the original hardcoded legal documents"""
        hardcoded_docs = [
            {
                "id": "family_law_1",
                "title": "Kodi i Familjes - Family Code",
                "source": "Albanian Family Code",
                "content": "Kodi i Familjes rregullon marrÃ«dhÃ«niet familjare, martesÃ«n, divorci, dhe tÃ« drejtat e fÃ«mijÃ«ve nÃ« ShqipÃ«ri.",
                "content_en": "The Family Code regulates family relationships, marriage, divorce, and children's rights in Albania.",
                "document_type": "hardcoded"
            },
            {
                "id": "property_law_1", 
                "title": "Ligji i PronÃ«sisÃ« - Property Law",
                "source": "Albanian Property Law",
                "content": "Ligji i pronÃ«sisÃ« nÃ« ShqipÃ«ri rregullon tÃ« drejtÃ«n e pronÃ«sisÃ« private, publike dhe transferimin e pronave.",
                "content_en": "Property law in Albania regulates private property rights, public property and property transfers.",
                "document_type": "hardcoded"
            },
            {
                "id": "labor_law_1",
                "title": "Kodi i PunÃ«s - Labor Code", 
                "source": "Albanian Labor Code",
                "content": "Kodi i PunÃ«s rregullon marrÃ«dhÃ«niet e punÃ«s, tÃ« drejtat dhe detyrimet e punÃ«dhÃ«nÃ«sve dhe tÃ« punÃ«suarve. Ã‡do punÃ«tor ka tÃ« drejtÃ« pÃ«r pushim vjetor me pagesÃ«. Pushimi vjetor Ã«shtÃ« gjithsej 28 ditÃ« kalendarike nÃ« vit, nga tÃ« cilat 14 ditÃ« janÃ« tÃ« detyrueshme dhe duhet tÃ« merren gjatÃ« vitit. DitÃ«t e tjera mund tÃ« transferohen nÃ« vitin tjetÃ«r me marrÃ«veshje me punÃ«dhÃ«nÃ«sin.",
                "content_en": "The Labor Code regulates employment relationships, rights and obligations of employers and employees. Every worker has the right to paid annual leave. Annual leave is a total of 28 calendar days per year, of which 14 days are mandatory and must be taken during the year. Other days can be transferred to the next year by agreement with the employer.",
                "document_type": "hardcoded"
            },
            {
                "id": "criminal_law_1",
                "title": "Kodi Penal - Criminal Code",
                "source": "Albanian Criminal Code", 
                "content": "Kodi Penal i ShqipÃ«risÃ« pÃ«rcakton veprat penale dhe sanksionet pÃ«rkatÃ«se pÃ«r to.",
                "content_en": "Albania's Criminal Code defines criminal acts and their corresponding sanctions.",
                "document_type": "hardcoded"
            },
            {
                "id": "business_law_1",
                "title": "Ligji pÃ«r Biznesin - Business Law",
                "source": "Albanian Business Law",
                "content": "Ligji pÃ«r biznesin rregullon themelimin, funksionimin dhe mbylljen e shoqÃ«rive tregtare nÃ« ShqipÃ«ri.",
                "content_en": "Business law regulates the establishment, operation and closure of commercial companies in Albania.",
                "document_type": "hardcoded"
            },
            {
                "id": "civil_code_1",
                "title": "Kodi Civil - Civil Code",
                "source": "Albanian Civil Code",
                "content": "Kodi Civil rregullon tÃ« drejtat dhe detyrimet civile, kontrata dhe pÃ«rgjegjÃ«sinÃ« civile.",
                "content_en": "The Civil Code regulates civil rights and obligations, contracts and civil liability.",
                "document_type": "hardcoded"
            },
            {
                "id": "constitution_1",
                "title": "Kushtetuta e ShqipÃ«risÃ« - Albanian Constitution", 
                "source": "Albanian Constitution",
                "content": "Kushtetuta Ã«shtÃ« ligji mÃ« i lartÃ« i vendit dhe garanton tÃ« drejtat dhe liritÃ« themelore tÃ« qytetarÃ«ve.",
                "content_en": "The Constitution is the highest law of the country and guarantees fundamental rights and freedoms of citizens.",
                "document_type": "hardcoded"
            },
            {
                "id": "tax_law_1",
                "title": "Ligji pÃ«r Taksat - Tax Law",
                "source": "Albanian Tax Law", 
                "content": "Ligji pÃ«r taksat rregullon sistemin tatimor, detyrimet tatimore dhe procedurat e mbledhjes sÃ« taksave.",
                "content_en": "Tax law regulates the tax system, tax obligations and tax collection procedures.",
                "document_type": "hardcoded"
            }
        ]
        
        return hardcoded_docs
    
    def _load_scraped_documents(self):
        """Load documents scraped from qbz.gov.al"""
        try:
            scraped_docs = self.scraper.get_processed_documents_for_rag()
            if not HEADLESS:
                st.info(f"ğŸ“„ Loaded {len(scraped_docs)} scraped document chunks")
            # Merge to super-chunks for speed and better cross-paragraph combinations
            merged = self._to_superchunks(scraped_docs, self.superchunk_chars, self.superchunk_overlap)
            if not HEADLESS:
                st.info(f"ğŸ§© Built {len(merged)} super-chunks for scraped docs (â‰ˆ{self.superchunk_chars} chars each)")
            return merged
        except Exception as e:
            if not HEADLESS:
                st.warning(f"âš ï¸ Could not load scraped documents: {e}")
            return []
    
    def _load_pdf_documents(self):
        """Load processed PDF documents"""
        try:
            pdf_file = os.path.join("legal_documents", "pdf_rag_documents.json")
            if os.path.exists(pdf_file):
                with open(pdf_file, 'r', encoding='utf-8') as f:
                    pdf_docs = json.load(f)
                
                # Normalize PDF documents to match expected format
                normalized_docs = []
                for doc in pdf_docs:
                    normalized_doc = {
                        'title': doc.get('title', 'Unknown Document'),
                        'content': doc.get('content', ''),
                        'source': doc.get('filename', doc.get('source_url', 'Local PDF')),
                        'url': doc.get('source_url', ''),
                        'document_type': 'local_pdf'
                    }
                    normalized_docs.append(normalized_doc)
                
                if not HEADLESS:
                    st.info(f"ğŸ“„ Loaded {len(normalized_docs)} PDF document chunks")
                # Merge into super-chunks for better semantic coverage
                merged = self._to_superchunks(normalized_docs, self.superchunk_chars, self.superchunk_overlap)
                if not HEADLESS:
                    st.info(f"ğŸ§© Built {len(merged)} super-chunks for PDF docs (â‰ˆ{self.superchunk_chars} chars each)")
                return merged
            else:
                if not HEADLESS:
                    st.warning("âš ï¸ No processed PDF documents found")
                return []
        except Exception as e:
            st.warning(f"âš ï¸ Could not load PDF documents: {e}")
            return []
    
    def _load_all_documents(self):
        """Load hardcoded, scraped, and PDF documents"""
        # Load hardcoded documents
        hardcoded = self._load_hardcoded_documents()
        
        # Load scraped documents
        scraped = self._load_scraped_documents()
        
        # Load PDF documents
        pdf_docs = self._load_pdf_documents()
        
        # Combine all documents
        self.legal_documents = hardcoded + scraped + pdf_docs
        # Ensure IDs exist for neighbor expansion and caching determinism
        for i, d in enumerate(self.legal_documents):
            if 'id' not in d or not d.get('id'):
                prefix = (d.get('source') or d.get('url') or d.get('title') or 'doc').replace(' ', '_')
                d['id'] = f"{prefix}_chunk_{i}"
        
        if not HEADLESS:
            st.info(
                f"ğŸ“š Total documents loaded: {len(self.legal_documents)} "
                f"({len(hardcoded)} hardcoded + {len(scraped)} scraped + {len(pdf_docs)} PDF)"
            )
    
    def _load_embeddings_cache(self):
        """Load cached embeddings if available"""
        cache_file = self.embeddings_cache_file if self.use_google_embeddings else 'document_embeddings_cache_local.json'
        
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                
                # Check if cache is still valid
                if (cache.get('document_count') == len(self.legal_documents) and 
                    cache.get('embedding_type') == ('google' if self.use_google_embeddings else 'local') and
                    cache.get('chunking', {}) == {
                        'superchunk_chars': self.superchunk_chars,
                        'superchunk_overlap': self.superchunk_overlap
                    }):
                    self.document_embeddings = np.array(cache['embeddings'])
                    if not HEADLESS:
                        st.success(f"âœ… Loaded cached {'Google' if self.use_google_embeddings else 'local'} embeddings")
                    return True
            except Exception as e:
                if not HEADLESS:
                    st.warning(f"âš ï¸ Could not load embedding cache: {e}")
        
        return False
    
    def _save_embeddings_cache(self):
        """Save embeddings to cache"""
        try:
            cache_file = self.embeddings_cache_file if self.use_google_embeddings else 'document_embeddings_cache_local.json'
            
            cache = {
                'embeddings': self.document_embeddings.tolist(),
                'document_count': len(self.legal_documents),
                'embedding_type': 'google' if self.use_google_embeddings else 'local',
                'created_at': datetime.now().isoformat(),
                'chunking': {
                    'superchunk_chars': self.superchunk_chars,
                    'superchunk_overlap': self.superchunk_overlap
                }
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache, f)
                
            if not HEADLESS:
                st.success(f"âœ… {'Google' if self.use_google_embeddings else 'Local'} embeddings cached for faster loading")
        except Exception as e:
            if not HEADLESS:
                st.warning(f"âš ï¸ Could not save embedding cache: {e}")
    
    def _generate_embeddings(self):
        """Generate embeddings for all documents using Google or local model"""
        if not self.use_google_embeddings and not self.model:
            if not HEADLESS:
                st.error("âŒ No embedding model available - cannot generate embeddings")
            return
        
        if not self.legal_documents:
            if not HEADLESS:
                st.error("âŒ No documents loaded - cannot generate embeddings")
            return
        
        # Try to load from cache first
        if self._load_embeddings_cache():
            return
        
        try:
            if self.use_google_embeddings:
                if not HEADLESS:
                    st.info("ğŸ”„ Generating Google embeddings for better Albanian legal search...")
                
                # Prepare document texts with enhanced content
                document_texts = []
                for doc in self.legal_documents:
                    # Combine title and content for better context
                    enhanced_text = f"Dokument ligjor: {doc['title']}. PÃ«rmbajtja: {doc['content']}"
                    if doc.get('content_en'):
                        enhanced_text += f" English: {doc['content_en']}"
                    document_texts.append(enhanced_text)
                
                # Generate embeddings using Google's model
                self.document_embeddings = self._get_google_embeddings(document_texts)
                
                if self.document_embeddings is not None:
                    self._save_embeddings_cache()
                    if not HEADLESS:
                        st.success(f"âœ… Generated Google embeddings for {len(self.legal_documents)} documents")
                else:
                    raise Exception("Failed to generate Google embeddings")
                    
            else:
                # Fallback to local embeddings
                if not HEADLESS:
                    st.info("ğŸ”„ Generating local embeddings...")
                
                document_texts = []
                for doc in self.legal_documents:
                    combined_text = f"{doc['content']} {doc.get('content_en', '')}"
                    document_texts.append(combined_text)
                
                self.document_embeddings = self.model.encode(document_texts)
                self._save_embeddings_cache()
                if not HEADLESS:
                    st.success(f"âœ… Generated local embeddings for {len(self.legal_documents)} documents")

            # Build sparse index for hybrid retrieval using the same document_texts prepared above
            try:
                if 'document_texts' not in locals():
                    # Re-create document_texts if not present
                    document_texts = []
                    for doc in self.legal_documents:
                        enhanced_text = f"{doc['title']} â€” {doc['content']} {doc.get('content_en', '')}"
                        document_texts.append(enhanced_text)
                self._build_sparse_index(document_texts)
            except Exception:
                pass
            
        except Exception as e:
            if not HEADLESS:
                st.error(f"âŒ Error generating embeddings: {e}")
            
            # Try fallback to local model if Google fails
            if self.use_google_embeddings and self.model:
                if not HEADLESS:
                    st.info("ğŸ”„ Trying local embeddings as fallback...")
                try:
                    self.use_google_embeddings = False
                    self._generate_embeddings()
                except Exception as fallback_error:
                    if not HEADLESS:
                        st.error(f"âŒ Fallback also failed: {fallback_error}")
                    self.document_embeddings = None
    
    def search_documents(self, query: str, top_k: int = 3, mode: str = 'hybrid', session_id: Optional[str] = None, multi_query: bool = True):
        """Search for relevant documents using hybrid retrieval and MMR diversification.

        Args:
            query: user query
            top_k: number of chunks to return
            mode: 'hybrid' | 'embedding' | 'sparse'
            session_id: optional conversation id for context-aware expansion
            multi_query: expand query into variants and aggregate
        """
        if self.document_embeddings is None:
            return []
        
        try:
            # Enhance Albanian queries for better semantic search
            enhanced_query = self._enhance_albanian_query(query)

            # Include brief memory to disambiguate retrieval (last 2 user turns)
            memory_context = self.get_memory_context(session_id) if session_id else ""
            if memory_context:
                enhanced_query = f"{enhanced_query} \nKontekst bisede: {memory_context}"
            
            # Generate query embedding using appropriate method
            if self.use_google_embeddings:
                query_embedding = self._get_google_query_embedding(enhanced_query)
                if query_embedding is None:
                    # Fallback to local model if available
                    if self.model:
                        query_embedding = self.model.encode([enhanced_query])
                    else:
                        if not HEADLESS:
                            st.error("âŒ No embedding method available for query")
                        return []
            else:
                if not self.model:
                    if not HEADLESS:
                        st.error("âŒ Local model not loaded")
                    return []
                query_embedding = self.model.encode([enhanced_query])
            
            # Calculate dense similarities
            dense_sim = cosine_similarity(query_embedding, self.document_embeddings)[0]

            # Calculate sparse similarities if enabled
            sparse_sim = None
            if self.tfidf_vectorizer is not None and self.tfidf_matrix is not None and mode in ('hybrid', 'sparse'):
                try:
                    q_vec = self.tfidf_vectorizer.transform([enhanced_query])
                    # cosine similarity for sparse vectors
                    # sklearn doesn't provide direct; use dot product as TF-IDF is L2-normalized by default
                    sparse_sim = (q_vec @ self.tfidf_matrix.T).toarray()[0]
                except Exception:
                    sparse_sim = None

            # Combine
            if mode == 'embedding' or sparse_sim is None:
                combined_sim = dense_sim
            elif mode == 'sparse':
                combined_sim = sparse_sim
            else:
                alpha = self.hybrid_alpha
                combined_sim = alpha * dense_sim + (1 - alpha) * sparse_sim
            
            # Multi-query expansion (lightweight): include original + 1-2 heuristic rewrites
            if multi_query:
                rewrites = self._expand_queries(enhanced_query)
                for rw in rewrites:
                    if rw.strip() == enhanced_query.strip():
                        continue
                    # Dense
                    if self.use_google_embeddings:
                        rw_emb = self._get_google_query_embedding(rw)
                        if rw_emb is None and self.model:
                            rw_emb = self.model.encode([rw])
                    else:
                        rw_emb = self.model.encode([rw]) if self.model else None
                    if rw_emb is not None:
                        rw_dense = cosine_similarity(rw_emb, self.document_embeddings)[0]
                        if mode == 'embedding' or (self.tfidf_vectorizer is None or self.tfidf_matrix is None):
                            combined_sim = np.maximum(combined_sim, rw_dense)
                        else:
                            # Sparse for rewrite
                            try:
                                qv = self.tfidf_vectorizer.transform([rw])
                                rw_sparse = (qv @ self.tfidf_matrix.T).toarray()[0]
                                alpha = self.hybrid_alpha if mode == 'hybrid' else 0.0
                                rw_comb = alpha * rw_dense + (1 - alpha) * rw_sparse
                                combined_sim = np.maximum(combined_sim, rw_comb)
                            except Exception:
                                combined_sim = np.maximum(combined_sim, rw_dense)

            # MMR selection to diversify
            selected_indices = self._mmr_select(query_embedding[0], self.document_embeddings, combined_sim, k=top_k, lambda_param=self.mmr_lambda)

            # Optionally expand with neighbors from same document (by chunk id suffix)
            expanded_indices = self._expand_neighbors(selected_indices, self.neighbor_expansion)
            
            results = []
            max_similarity = 0
            
            # Find the maximum similarity to make adaptive threshold decisions
            for idx in expanded_indices:
                similarity = float(combined_sim[idx])
                if similarity > max_similarity:
                    max_similarity = similarity
                
                # Use adaptive threshold with improved settings for Google embeddings
                base_threshold = self.similarity_threshold
                if self.use_google_embeddings:
                    # Google embeddings have different scale, adjust threshold
                    base_threshold = max(0.1, self.similarity_threshold * 0.8)
                
                adaptive_threshold = max(base_threshold, max_similarity * 0.7) if max_similarity > 0.2 else base_threshold
                
                if similarity > adaptive_threshold or (len(results) == 0 and idx == expanded_indices[0] and similarity > 0.1):
                    doc = self.legal_documents[idx].copy()
                    doc['similarity'] = similarity
                    results.append(doc)
            
            # Store max similarity for fallback decision
            self.last_search_max_similarity = max_similarity
            
            return results
            
        except Exception as e:
            if not HEADLESS:
                st.error(f"âŒ Search error: {e}")
            return []

    def _expand_neighbors(self, indices: List[int], window: int) -> List[int]:
        """Expand selected indices by including +/- window neighbors within same document chunks when ids suggest ordering."""
        if window <= 0:
            return indices
        expanded: set[int] = set(indices)
        for idx in indices:
            base_id = self.legal_documents[idx].get('id', '')
            # Extract numeric chunk suffix if present
            chunk_num = None
            prefix = None
            if '_chunk_' in base_id:
                try:
                    chunk_num = int(base_id.split('_chunk_')[-1])
                    prefix = base_id.rsplit('_chunk_', 1)[0]
                except Exception:
                    chunk_num = None
            if chunk_num is not None and prefix is not None:
                # search neighbors in list for same prefix and num +/- window
                for j, d in enumerate(self.legal_documents):
                    did = d.get('id', '')
                    if did.startswith(prefix + '_chunk_'):
                        try:
                            n = int(did.split('_chunk_')[-1])
                            if abs(n - chunk_num) <= window:
                                expanded.add(j)
                        except Exception:
                            continue
        return list(expanded)

    def _mmr_select(self, query_vec: np.ndarray, doc_embs: np.ndarray, sim_scores: np.ndarray, k: int, lambda_param: float = 0.5) -> List[int]:
        """Maximal Marginal Relevance selection to diversify top-k results."""
        if k <= 0:
            return []
        selected: List[int] = []
        candidates = set(range(len(sim_scores)))
        # Precompute doc-doc similarities lazily when needed
        doc_norms = np.linalg.norm(doc_embs, axis=1) + 1e-10
        while len(selected) < k and candidates:
            if not selected:
                # pick max sim
                idx = int(np.argmax(sim_scores))
                selected.append(idx)
                candidates.discard(idx)
                continue
            best_idx: Optional[int] = None
            best_score = -1e9
            for i in list(candidates):
                # relevance
                rel = sim_scores[i]
                # redundancy: max similarity to already selected
                red = 0.0
                for s in selected:
                    # cosine between doc i and doc s using precomputed embs
                    dot = float(np.dot(doc_embs[i], doc_embs[s]))
                    sim = dot / (doc_norms[i] * doc_norms[s])
                    if sim > red:
                        red = sim
                mmr_score = lambda_param * rel - (1 - lambda_param) * red
                if mmr_score > best_score:
                    best_score = mmr_score
                    best_idx = i
            if best_idx is None:
                break
            selected.append(best_idx)
            candidates.discard(best_idx)
        return selected

    def _expand_queries(self, enhanced_query: str) -> List[str]:
        """Heuristic multi-query rewrites to improve recall without extra LLM calls."""
        q = enhanced_query
        rewrites = []
        # Swap some common Albanian synonyms order
        rewrites.append(q.replace('procedurÃ«', 'hapa'))
        rewrites.append(q.replace('dÃ«nim', 'sanksion'))
        rewrites.append(q.replace('pagÃ«', 'rrogÃ«'))
        # English hint variant
        rewrites.append(q + " in English: law code article section penalty definition procedure")
        # Deduplicate and limit
        uniq: List[str] = []
        for r in rewrites:
            if r not in uniq:
                uniq.append(r)
        return uniq[:3]
    
    def _enhance_albanian_query(self, query: str) -> str:
        """Enhance Albanian queries with comprehensive legal synonyms and context"""
        enhanced_query = query.lower()
        
        # Comprehensive Albanian legal synonyms for better semantic matching
        legal_synonyms = {
            # Employment & Labor Law
            'punÃ«': 'punÃ« work employment job labor pune punim',
            'pagÃ«': 'pagÃ« salary wage rrogÃ« compensation pagesÃ«',
            'rrogÃ«': 'rrogÃ« salary wage pagÃ« income tÃ« ardhura',
            'minimum': 'minimum minim minimale lowest mÃ« tÃ« ulÃ«t bazÃ«',
            'pushim': 'pushim vacation leave holiday pushimi ditÃ«',
            'punÃ«tor': 'punÃ«tor employee worker staff punonjÃ«s',
            'punÃ«dhÃ«nÃ«s': 'punÃ«dhÃ«nÃ«s employer boss company kompani',
            
            # Criminal Law - Enhanced for better matching
            'dÃ«nim': 'dÃ«nim punishment penalty sanksion gjykim burgim gjoba',
            'denimi': 'denimi punishment penalty sanksion gjykim burgim gjoba',
            'plagosje': 'plagosje injury wound attack assault sulm dhunÃ«',
            'plagos': 'plagos injure wound attack assault sulm dhunÃ«',
            'armÃ«': 'armÃ« weapon tool vegÃ«l instrument mjete ftohtÃ«',
            'arme': 'arme weapon tool vegÃ«l instrument mjete ftohtÃ«',
            'ftohtÃ«': 'ftohtÃ« cold steel metalike hekur thikÃ«',
            'vjedhje': 'vjedhje theft stealing robbery grabitje marrje',
            'sanksion': 'sanksion penalty punishment dÃ«nim gjobÃ« burgim',
            'gjobÃ«': 'gjobÃ« fine penalty sanksion dÃ«nim financiar',
            'krim': 'krim crime criminal penal vepÃ«r penale kundÃ«rvajtje',
            'penal': 'penal criminal crime kod ligj sanksion dÃ«nim',
            'burgim': 'burgim prison jail detention arrest paraburgim',
            'vrasje': 'vrasje murder killing homicide vdekje ekzekutim',
            'dhunÃ«': 'dhunÃ« violence force brutality aggression sulm',
            'sulm': 'sulm attack assault agression dhunÃ«',
            
            # Business & Commercial Law
            'kompani': 'kompani company business shoqÃ«ri enterprise biznes',
            'biznes': 'biznes business company kompani shoqÃ«ri tregtare',
            'regjistroj': 'regjistroj register establish themelon krijoj',
            'shoqÃ«ri': 'shoqÃ«ri company business enterprise kompani',
            'tregtare': 'tregtare commercial business trading biznes',
            
            # Family Law
            'martesÃ«': 'martesÃ« marriage wedding bashkÃ«short familje',
            'divorcÃ«': 'divorcÃ« divorce separation ndarje',
            'fÃ«mijÃ«': 'fÃ«mijÃ« child children kids tÃ« mitur',
            'familje': 'familje family household shtÃ«pi',
            'prindÃ«r': 'prindÃ«r parents mother father',
            
            # Civil Law
            'kontrata': 'kontrata contract agreement marrÃ«veshje',
            'pronÃ«': 'pronÃ« property asset pasuri',
            'tÃ« drejta': 'tÃ« drejta rights legal ligjore',
            'detyrim': 'detyrim obligation duty responsibility',
            
            # General Legal Terms
            'ligj': 'ligj law legal kod juridik ligjor',
            'kod': 'kod code law ligj juridik',
            'juridik': 'juridik legal law ligj ligjor',
            'gjykatÃ«': 'gjykatÃ« court tribunal drejtÃ«si',
            'avokat': 'avokat lawyer attorney jurist',
            
            # Tax & Finance
            'taksÃ«': 'taksÃ« tax duty detyrim tatim',
            'tatim': 'tatim tax taksÃ« detyrim financiar',
            
            # Constitutional & Administrative
            'kushtetutÃ«': 'kushtetutÃ« constitution basic law',
            'shtet': 'shtet state government qeveri',
            'administrativ': 'administrativ administrative government publik'
        }
        
        # Add relevant synonyms to enhance query
        for albanian_term, synonyms in legal_synonyms.items():
            if albanian_term in enhanced_query:
                enhanced_query += f" {synonyms}"
        
        # Add legal context keywords based on query type
        if any(word in enhanced_query for word in ['sa', 'how much', 'shumÃ«', 'amount']):
            enhanced_query += " sasi amount vlerÃ«"
        
        if any(word in enhanced_query for word in ['si', 'how', 'procedura', 'process']):
            enhanced_query += " procedurÃ« process hapa steps"
        
        if any(word in enhanced_query for word in ['Ã§farÃ«', 'what', 'cilat', 'which']):
            enhanced_query += " pÃ«rkufizim definition detaje specifics"
        
        return enhanced_query
    
    def search_legal_documents(self, query: str, session_id: Optional[str] = None) -> str:
        """Main interface method for searching and responding to legal queries"""
        try:
            # Search for relevant documents
            relevant_docs = self.search_documents(query, top_k=self.max_chunks_to_return, mode='hybrid', session_id=session_id)
            
            # Compact context to fit budget and accelerate LLM
            docs_for_llm = self._compact_docs(relevant_docs, query, self.max_context_length) if self.context_packing else relevant_docs
            
            # Generate comprehensive response
            response = self.generate_response(query, docs_for_llm, session_id=session_id)

            # Persist in-memory conversation if session provided
            if session_id:
                self.add_to_memory(session_id, role='user', content=query)
                self.add_to_memory(session_id, role='assistant', content=response)
            
            return response
            
        except Exception as e:
            if not HEADLESS:
                st.error(f"âŒ Search error: {e}")
            return f"âŒ **Gabim nÃ« kÃ«rkim**: {e}\n\nâš–ï¸ Ju lutem provoni pÃ«rsÃ«ri ose kontaktoni njÃ« jurist."
    
    def _find_similar_documents(self, query: str, top_k: int = 5):
        """Find similar documents - wrapper around search_documents for compatibility"""
        docs = self.search_documents(query, top_k)
        
        # Return in the format expected by test files: (doc, similarity) tuples
        return [(doc, doc.get('similarity', 0.0)) for doc in docs]
    
    def generate_response(self, query: str, relevant_docs: list, session_id: Optional[str] = None) -> str:
        """Generate a comprehensive response using cloud LLM or fallback"""
        # Build memory context for LLM
        memory_context = self.get_memory_context(session_id) if session_id else ""
        # Trim memory context to keep prompts small
        if len(memory_context) > 1000:
            memory_context = memory_context[-1000:]
        if self.cloud_llm and self.cloud_llm.api_key:
            try:
                # Use cloud LLM for advanced response generation
                return self.cloud_llm.generate_response(query, relevant_docs, context=memory_context)
            except Exception as e:
                if not HEADLESS:
                    st.warning(f"âš ï¸ Cloud LLM error, using fallback: {e}")
                return self._fallback_response(query, relevant_docs)
        else:
            # Use fallback response
            return self._fallback_response(query, relevant_docs)

    # -----------------------------
    # Super-chunking and context packing helpers
    # -----------------------------
    def _to_superchunks(self, docs: List[Dict], chunk_chars: int, overlap_chars: int) -> List[Dict]:
        """Merge chunks into larger super-chunks, grouped per source/filename for coherence."""
        if chunk_chars <= 0 or not docs:
            return docs
        # Group docs by a stable key (source or title)
        groups: Dict[str, List[Dict]] = {}
        for d in docs:
            key = (d.get('source') or d.get('url') or d.get('title') or 'doc').strip()
            groups.setdefault(key, []).append(d)
        merged: List[Dict] = []
        for key, items in groups.items():
            # keep order
            buffer = ""
            meta = items[0]
            part = 0
            def flush(buf: str, m: Dict, p: int):
                if not m or not buf:
                    return
                item = {
                    'title': m.get('title', 'Document'),
                    'content': buf,
                    'source': m.get('source', key),
                    'url': m.get('url', ''),
                    'document_type': m.get('document_type', 'local_pdf'),
                    'id': f"{(key or 'doc').replace(' ', '_')}_chunk_{p}"
                }
                merged.append(item)
            for d in items:
                text = d.get('content', '')
                if not buffer:
                    buffer = text
                else:
                    buffer += "\n\n" + text
                while len(buffer) > chunk_chars:
                    to_take = buffer[:chunk_chars]
                    flush(to_take.strip(), meta, part)
                    part += 1
                    start = max(0, len(to_take) - overlap_chars)
                    buffer = buffer[start:]
            flush(buffer.strip(), meta, part)
        return merged

    def _compact_docs(self, docs: List[Dict], query: str, budget_chars: int) -> List[Dict]:
        """Extractive packing: select top sentences from each doc to fit within a character budget.

        Keeps document metadata; reduces content length to accelerate LLM and reduce tokens.
        """
        if budget_chars <= 0 or not docs:
            return docs
        # Split into sentences (simple heuristic)
        packed: List[Dict] = []
        remaining = max(500, budget_chars)  # ensure some reasonable space
        # Distribute budget roughly evenly across docs
        per_doc = max(250, remaining // max(1, len(docs)))
        for d in docs:
            content = d.get('content', '')
            excerpt = self._select_top_sentences(content, query, max_chars=per_doc)
            new_d = d.copy()
            if excerpt:
                new_d['content'] = excerpt
            packed.append(new_d)
        return packed

    def _select_top_sentences(self, text: str, query: str, max_chars: int = 600) -> str:
        """Score sentences by TF-IDF against the query and return the best within max_chars."""
        if not text:
            return text
        # Naive sentence split
        sentences = [s.strip() for s in text.replace('\n', ' ').split('.') if s.strip()]
        if not sentences:
            return text[:max_chars]
        # Build corpus of sentences + query
        corpus = sentences + [query]
        try:
            vec = TfidfVectorizer(max_features=5000, ngram_range=(1, 2)).fit_transform(corpus)
            q_vec = vec[-1]
            s_mat = vec[:-1]
            scores = (s_mat @ q_vec.T).toarray().ravel()
        except Exception:
            # Fallback: length-based
            scores = np.array([len(s) for s in sentences], dtype=float)
        order = np.argsort(-scores)
        picked: List[str] = []
        total = 0
        for idx in order:
            s = sentences[idx]
            if s.endswith('.'):
                sent = s
            else:
                sent = s + '.'
            if total + len(sent) > max_chars:
                if total < max_chars * 0.6:  # allow one longer sentence to avoid empty
                    picked.append(sent[: max(0, max_chars - total)])
                break
            picked.append(sent)
            total += len(sent)
            if total >= max_chars:
                break
        return ' '.join(picked) if picked else text[:max_chars]

    # -----------------------------
    # Conversation memory
    # -----------------------------
    def add_to_memory(self, session_id: str, role: str, content: str, max_turns: int = 10):
        if not session_id:
            return
        hist = self.memory_store.setdefault(session_id, [])
        hist.append({"role": role, "content": content})
        # cap memory
        if len(hist) > 2 * max_turns:
            self.memory_store[session_id] = hist[-2 * max_turns :]

    def get_memory_context(self, session_id: Optional[str], last_n: int = 4) -> str:
        if not session_id:
            return ""
        hist = self.memory_store.get(session_id, [])
        if not hist:
            return ""
        # Take last_n turns
        snippet = hist[-last_n:]
        formatted = []
        for h in snippet:
            role = 'PÃ«rdoruesi' if h['role'] == 'user' else 'Asistenti'
            formatted.append(f"{role}: {h['content']}")
        return "\n".join(formatted)
    
    def _fallback_response(self, query: str, relevant_docs: list) -> str:
        """Enhanced fallback response with intelligent general legal knowledge"""
        
        # If we have relevant documents with decent similarity, use them
        if relevant_docs and any(doc.get('similarity', 0) > 0.3 for doc in relevant_docs):
            return self._generate_document_based_response(query, relevant_docs)
        
        # If no good documents found, provide intelligent general legal guidance
        return self._generate_general_legal_response(query)
    
    def _generate_document_based_response(self, query: str, relevant_docs: list) -> str:
        """Generate response based on found documents"""
        response = "ğŸ“‹ **PÃ«rgjigje bazuar nÃ« dokumentet ligjore shqiptare:**\n\n"
        
        # Analyze query to provide more specific introduction
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['sa', 'how much', 'shumÃ«']):
            response += "ğŸ’° **Informacion mbi sasi dhe vlera:**\n\n"
        elif any(word in query_lower for word in ['si', 'how', 'procedura']):
            response += "ğŸ“‹ **Procedura dhe hapa:**\n\n"
        elif any(word in query_lower for word in ['Ã§farÃ«', 'what', 'cilat']):
            response += "ğŸ“– **PÃ«rkufizime dhe detaje:**\n\n"
        else:
            response += "âš–ï¸ **Informacion ligjor:**\n\n"
        
        # Add document information
        for i, doc in enumerate(relevant_docs, 1):
            similarity = doc.get('similarity', 0)
            
            # Determine document source emoji
            if doc.get('document_type') == 'local_pdf':
                doc_emoji = "ğŸ“„"
            elif doc.get('document_type') == 'scraped_legal_document':
                doc_emoji = "ğŸŒ"
            else:
                doc_emoji = "ğŸ“š"
            
            response += f"{doc_emoji} **{doc.get('title', 'Dokument Ligjor')}** (Relevanca: {similarity:.0%})\n"
            
            # Add content excerpt
            content = doc.get('content', '')
            if content:
                # Truncate content intelligently
                if len(content) > 300:
                    content = content[:300] + "..."
                response += f"ğŸ“– {content}\n\n"
        
        response += "---\n"
        response += "âš–ï¸ **ShÃ«nim**: Ky informacion Ã«shtÃ« pÃ«r qÃ«llime informuese. "
        response += "PÃ«r kÃ«shilla tÃ« detajuara ligjore, konsultohuni me njÃ« jurist tÃ« kualifikuar."
        
        return response
    
    def _generate_general_legal_response(self, query: str) -> str:
        """Generate intelligent general legal response when no specific documents match"""
        query_lower = query.lower()
        
        # Salary/wage related queries
        if any(word in query_lower for word in ['salary', 'pagÃ«', 'rrogÃ«', 'minimum', 'minim']):
            return self._get_salary_general_response()
        
        # Employment/labor queries
        elif any(word in query_lower for word in ['punÃ«', 'employment', 'work', 'job', 'pushim', 'vacation']):
            return self._get_employment_general_response()
        
        # Business registration queries
        elif any(word in query_lower for word in ['kompani', 'biznes', 'regjistroj', 'business', 'company']):
            return self._get_business_general_response()
        
        # Criminal law queries
        elif any(word in query_lower for word in ['vjedhje', 'theft', 'krim', 'crime', 'sanksion', 'penalty']):
            return self._get_criminal_general_response()
        
        # Family law queries
        elif any(word in query_lower for word in ['martesÃ«', 'marriage', 'divorcÃ«', 'divorce', 'familje', 'family']):
            return self._get_family_general_response()
        
        # General legal guidance
        else:
            return self._get_generic_legal_response(query)
    
    def _get_salary_general_response(self) -> str:
        """General response for salary/wage questions"""
        return """
ğŸ’° **Paga Minimale nÃ« ShqipÃ«ri - Informacion i PÃ«rgjithshÃ«m**

ğŸ“Š **Paga minimale aktuale nÃ« ShqipÃ«ri**:
â€¢ **34,000 lekÃ«** nÃ« muaj (rreth 340 Euro)
â€¢ Kjo Ã«shtÃ« paga minimale pÃ«r punÃ«torÃ«t me kohÃ« tÃ« plotÃ«
â€¢ PÃ«rditÃ«sohet periodikisht nga Qeveria

ğŸ“‹ **Detaje tÃ« rÃ«ndÃ«sishme**:
â€¢ TÃ« gjithÃ« punÃ«torÃ«t kanÃ« tÃ« drejtÃ« pÃ«r pagÃ«n minimale
â€¢ PunÃ«dhÃ«nÃ«si nuk mund tÃ« paguajÃ« mÃ« pak se paga minimale
â€¢ Kjo pÃ«rfshin tÃ« gjitha kontributet dhe taksat

âš–ï¸ **BazÃ« ligjore**:
â€¢ Rregullohet nga Kodi i PunÃ«s i ShqipÃ«risÃ«
â€¢ Vendime specifike tÃ« KÃ«shillit tÃ« Ministrave
â€¢ Ligje tÃ« veÃ§anta pÃ«r sektorÃ« tÃ« ndryshÃ«m

ğŸ’¡ **PÃ«r informacion tÃ« pÃ«rditÃ«suar**:
â€¢ Kontaktoni Inspektoratin e PunÃ«s
â€¢ Consultoni me njÃ« jurist tÃ« specializuar
â€¢ Vizitoni faqen zyrtare tÃ« MinistrisÃ« sÃ« Financave

---
âš ï¸ **ShÃ«nim**: Informacioni mund tÃ« jetÃ« ndryshuar. Verifikoni me burime zyrtare.
"""
    
    def _get_employment_general_response(self) -> str:
        """General response for employment questions"""
        return """
ğŸ’¼ **TÃ« Drejtat e PunÃ«torÃ«ve nÃ« ShqipÃ«ri**

ğŸ“ **TÃ« drejta themelore**:
â€¢ Kontrata e punÃ«s me shkrim
â€¢ PagesÃ« pÃ«r punÃ«n e kryer
â€¢ Ambiente tÃ« sigurta pune
â€¢ Pushim vjetor me pagesÃ« (28 ditÃ«)
â€¢ Pushim javor (24 orÃ« tÃ« njÃ«pasnjÃ«shme)

â° **KohÃ« pune**:
â€¢ Maksimumi 8 orÃ« nÃ« ditÃ«, 40 orÃ« nÃ« javÃ«
â€¢ Pauza gjatÃ« ditÃ«s sÃ« punÃ«s
â€¢ Pagesa shtesÃ« pÃ«r orÃ«t jashtÃ« programit

ğŸ¥ **Sigurime dhe mbrojtje**:
â€¢ Sigurimi shÃ«ndetÃ«sor
â€¢ Sigurime shoqÃ«rore
â€¢ Kompensim pÃ«r aksidente nÃ« punÃ«
â€¢ Lejet e sÃ«mundjes

âš–ï¸ **BazÃ« ligjore**: Kodi i PunÃ«s i ShqipÃ«risÃ«

ğŸ’¡ **PÃ«r probleme**: Kontaktoni Inspektoratin e PunÃ«s ose njÃ« jurist.
"""
    
    def _get_business_general_response(self) -> str:
        """General response for business registration questions"""
        return """
ğŸ¢ **Regjistrimi i Biznesit nÃ« ShqipÃ«ri**

ğŸ“‹ **Hapat kryesorÃ«**:
1. **Zgjedhja e emrit** tÃ« biznesit
2. **PÃ«rcaktimi i formÃ«s juridike** (SHPK, SHA, etj.)
3. **Regjistrimi nÃ« QKB** (Qendra KombÃ«tare e Biznesit)
4. **Marrja e licencave** tÃ« nevojshme
5. **Regjistrimi tatimor** nÃ« DrejtorinÃ« e Tatimeve

ğŸ’° **Kostot**:
â€¢ Taksa e regjistrimit: rreth 2,000-5,000 lekÃ«
â€¢ Kosto notari (nÃ«se nevojitet)
â€¢ Taksa pÃ«r licencat specifike

ğŸ“ **Ku tÃ« shkoni**:
â€¢ **QKB** - Qendra KombÃ«tare e Biznesit
â€¢ **Zyrat e bashkive** pÃ«r licencat lokale
â€¢ **Drejtoria e Tatimeve** pÃ«r regjistrimin tatimor

â±ï¸ **KohÃ«zgjatja**: 1-3 ditÃ« pune pÃ«r procedurat bazÃ«

ğŸ’¡ **KÃ«shillÃ«**: Konsultohuni me njÃ« kontabilist ose jurist pÃ«r procedurat specifike.
"""
    
    def _get_criminal_general_response(self) -> str:
        """General response for criminal law questions"""
        return """
âš–ï¸ **Ligji Penal nÃ« ShqipÃ«ri - Informacion i PÃ«rgjithshÃ«m**

ğŸ“– **Kodi Penal** pÃ«rcakton:
â€¢ Veprat penale dhe sanksionet
â€¢ Procedurat gjyqÃ«sore
â€¢ TÃ« drejtat e tÃ« akuzuarit
â€¢ Llojet e dÃ«nimeve

ğŸ—¡ï¸ **Plagosje me ArmÃ« tÃ« FtohtÃ«**:
â€¢ **Plagosje e thjeshtÃ«**: Burgim deri nÃ« 2 vjet ose gjobÃ«
â€¢ **Plagosje e rÃ«ndÃ«**: Burgim nga 2 deri nÃ« 8 vjet
â€¢ **Plagosje shumÃ« tÃ« rÃ«ndÃ«**: Burgim nga 5 deri nÃ« 15 vjet
â€¢ **Me pasoja vdekjeje**: Burgim nga 10 deri nÃ« 20 vjet

âš”ï¸ **Mbajtja e ArmÃ«ve tÃ« FtohtÃ«**:
â€¢ Gjoba ose burgim deri nÃ« 3 muaj pÃ«r mbajtje pa leje
â€¢ PÃ«rdorimi nÃ« vepra penale rÃ«ndon dÃ«nimin

ğŸ›ï¸ **Llojet e dÃ«nimeve**:
â€¢ **Burgim** (nga 15 ditÃ« deri nÃ« 25 vjet)
â€¢ **Gjoba** (sasi tÃ« ndryshme)
â€¢ **PunÃ« nÃ« dobi tÃ« pÃ«rgjithshme**
â€¢ **Heqja e tÃ« drejtave** civile

âš ï¸ **Vepra tÃ« rÃ«nda**:
â€¢ Vrasja, grabitja, trafikimi
â€¢ Korrupsioni, pastrimi i parave
â€¢ Vepra kundÃ«r sigurisÃ« publike

ğŸ” **Procedura penale**:
â€¢ Hetimi, akuzimi, gjykimi
â€¢ E drejta pÃ«r mbrojtje ligjore
â€¢ Ankimimi i vendimeve

ğŸ’¡ **PÃ«r ndihmÃ« ligjore**: Kontaktoni njÃ« avokat penal tÃ« kualifikuar.

---
âš ï¸ **ShÃ«nim**: DÃ«nimet e sakta varen nga rrethanat e veÃ§anta tÃ« rastit. Konsultohuni me njÃ« jurist pÃ«r raste specifike.
"""
    
    def _get_family_general_response(self) -> str:
        """General response for family law questions"""
        return """
ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ **E Drejta e Familjes nÃ« ShqipÃ«ri**

ğŸ’ **Martesa**:
â€¢ Mosha minimale: 18 vjet (me pÃ«rjashtime 16 vjet)
â€¢ Dokumentet e nevojshme
â€¢ Ceremonia civile (e detyrueshme)
â€¢ Ceremonia fetare (opsionale)

ğŸ’” **Divorci**:
â€¢ **Me marrÃ«veshje**: procedurÃ« e thjeshtÃ«
â€¢ **Me mosmarrÃ«veshje**: procedurÃ« gjyqÃ«sore
â€¢ Ndarje e pasurisÃ« sÃ« pÃ«rbashkÃ«t
â€¢ Kujdestaria e fÃ«mijÃ«ve

ğŸ‘¶ **TÃ« drejtat e fÃ«mijÃ«ve**:
â€¢ E drejta pÃ«r kujdes dhe edukim
â€¢ Alimentet nga prindÃ«rit
â€¢ Mbrojtja nga dhuna
â€¢ PÃ«rfaqÃ«simi ligjor

ğŸ  **Pasuria familjare**:
â€¢ Pasuria e pÃ«rbashkÃ«t dhe individuale
â€¢ TrashÃ«gimia dhe testamenti
â€¢ TÃ« drejtat e banimit

âš–ï¸ **BazÃ« ligjore**: Kodi i Familjes sÃ« ShqipÃ«risÃ«

ğŸ’¡ **PÃ«r ndihmÃ«**: Konsultohuni me njÃ« jurist familjar.
"""
    
    def _get_generic_legal_response(self, query: str) -> str:
        """Generic legal response for unmatched queries"""
        return f"""
ğŸ¤” **Pyetja juaj**: "{query}"

âš–ï¸ **Sistemit Ligjor Shqiptar** pÃ«rfshin:

ğŸ“š **Kodet kryesore**:
â€¢ **Kodi Civil** - tÃ« drejtat civile, kontratat
â€¢ **Kodi Penal** - veprat penale, sanksionet  
â€¢ **Kodi i PunÃ«s** - marrÃ«dhÃ«niet e punÃ«s
â€¢ **Kodi i Familjes** - martesa, divorci, fÃ«mijÃ«t
â€¢ **Kodi Doganor** - importi, eksporti
â€¢ **Kushtetuta** - tÃ« drejtat themelore

ğŸ›ï¸ **Institucionet kryesore**:
â€¢ Gjykatat e shkallÃ«s sÃ« parÃ« dhe tÃ« dytÃ«
â€¢ Gjykata e LartÃ«
â€¢ Gjykata Kushtetuese
â€¢ Prokuroria e PÃ«rgjithshme

ğŸ’¡ **PÃ«r pyetjen tuaj specifike**:
â€¢ Reformuloni pyetjen me fjalÃ« kyÃ§e tÃ« qarta
â€¢ PÃ«rdorni terma specifike ligjorÃ«
â€¢ Kontaktoni njÃ« jurist tÃ« specializuar
â€¢ Vizitoni faqen qbz.gov.al pÃ«r ligjet e plota

ğŸ“ **NdihmÃ« ligjore falas**: 0800 8080 (Qendra e NdihmÃ«s Ligjore)

---
âš ï¸ **ShÃ«nim**: PÃ«r kÃ«shilla tÃ« detajuara ligjore, konsultohuni me njÃ« jurist tÃ« kualifikuar.
"""
    
    def _get_vacation_response(self, relevant_docs: list) -> str:
        """Generate specific response for vacation-related queries"""
        response = "ğŸ–ï¸ **Informacion mbi Pushimin Vjetor nÃ« ShqipÃ«ri**\n\n"
        
        response += "Sipas Kodit tÃ« PunÃ«s sÃ« ShqipÃ«risÃ«:\n\n"
        response += "ğŸ“… **Pushimi Vjetor Total**: 28 ditÃ« kalendarike nÃ« vit\n"
        response += "âš ï¸ **Pushimi i DetyrueshÃ«m**: 14 ditÃ« qÃ« duhet tÃ« merren gjatÃ« vitit\n"
        response += "ğŸ”„ **Pushimi Opsional**: 14 ditÃ« tÃ« tjera mund tÃ« transferohen nÃ« vitin tjetÃ«r\n"
        response += "ğŸ’° **Pushimi me PagesÃ«**: Po, pushimi vjetor Ã«shtÃ« me pagesÃ« tÃ« plotÃ«\n\n"
        
        response += "ğŸ“‹ **Detaje tÃ« RÃ«ndÃ«sishme**:\n"
        response += "â€¢ TÃ« gjithÃ« punÃ«torÃ«t kanÃ« tÃ« drejtÃ« pÃ«r pushim vjetor\n"
        response += "â€¢ Pushimi planifikohet nÃ« marrÃ«veshje me punÃ«dhÃ«nÃ«sin\n"
        response += "â€¢ 14 ditÃ«t e detyrueshme nuk mund tÃ« zÃ«vendÃ«sohen me pagesÃ«\n"
        response += "â€¢ DitÃ«t e tjera mund tÃ« akumulohen me marrÃ«veshje\n\n"
        
        # Add relevant document excerpts if available
        if relevant_docs:
            for doc in relevant_docs[:2]:
                if any(term in doc.get('content', '').lower() for term in ['pushim', 'vacation', 'punÃ«', 'labor']):
                    doc_type = "ğŸŒ" if doc.get('document_type') == 'scraped_legal_document' else "ğŸ“š"
                    response += f"{doc_type} **Burim**: {doc['title']}\n"
                    response += f"ğŸ“– {doc['content'][:200]}...\n\n"
        
        response += "\n---\nâš–ï¸ **KÃ«shillÃ« Ligjore**: PÃ«r situata tÃ« veÃ§anta ose mosmarrÃ«veshje, "
        response += "konsultohuni me njÃ« jurist tÃ« specializuar nÃ« tÃ« drejtÃ«n e punÃ«s."
        
        return response
    
    def _get_no_results_response(self) -> str:
        """Response when no relevant documents are found"""
        return """
        ğŸ¤” **Nuk u gjetÃ«n dokumente specifike pÃ«r kÃ«tÃ« pyetje.**
        
        Ju lutem provoni:
        â€¢ PÃ«rdorni fjalÃ« kyÃ§e tÃ« tjera
        â€¢ BÃ«ni pyetjen mÃ« tÃ« qartÃ«
        â€¢ Kontrolloni drejtshkrimin
        
        ğŸ“š Ose eksploro temat e disponueshme:
        â€¢ Kodi i Familjes
        â€¢ Ligji i PronÃ«sisÃ«  
        â€¢ Kodi i PunÃ«s
        â€¢ Kodi Penal
        â€¢ Ligji pÃ«r Biznesin
        â€¢ Kodi Civil
        â€¢ Kushtetuta e ShqipÃ«risÃ«
        â€¢ Ligji pÃ«r Taksat
        
        Gjithashtu disponojmÃ« dokumente tÃ« pÃ«rditÃ«suara nga qbz.gov.al
        """
    
    def scrape_new_documents(self):
        """Scrape new documents and update the system"""
        with st.spinner("ğŸ”„ Duke shkarkuar dokumente tÃ« reja nga qbz.gov.al..."):
            try:
                # Scrape new documents
                found_docs = self.scraper.scrape_qbz_search(['ligj', 'kod', 'vendim'])
                
                if found_docs:
                    st.success(f"ğŸ“¥ U shkarkuan {len(found_docs)} dokumente tÃ« reja")
                    
                    # Process the documents
                    with st.spinner("ğŸ“– Duke pÃ«rpunuar dokumentet..."):
                        processed = self.scraper.process_downloaded_documents()
                        st.success(f"âœ… U pÃ«rpunuan {len(processed)} dokumente")
                    
                    # Reload all documents
                    self._load_all_documents()
                    
                    # Regenerate embeddings
                    self._generate_embeddings()
                    
                    st.success("ğŸ‰ Sistemi u pÃ«rditÃ«sua me dokumente tÃ« reja!")
                    return True
                else:
                    st.info("â„¹ï¸ Nuk u gjetÃ«n dokumente tÃ« reja")
                    return False
                    
            except Exception as e:
                st.error(f"âŒ Gabim gjatÃ« shkarkimit: {e}")
                return False


def main():
    """Main Streamlit application"""
    st.set_page_config(
        page_title="Albanian Legal RAG System", 
        page_icon="âš–ï¸",
        layout="wide"
    )
    
    # Header
    st.title("âš–ï¸ Sistemi Shqiptar i Ligjit me AI")
    st.markdown("### ğŸ¤– Intelligent Legal Research Assistant for Albanian Law")
    st.markdown("#### ğŸŒ Now powered by advanced cloud AI (Groq API)")
    
    # Initialize session state
    if 'rag_system' not in st.session_state:
        with st.spinner("ğŸ”„ Duke inicializuar sistemin..."):
            st.session_state.rag_system = CloudEnhancedAlbanianLegalRAG()
    
    rag_system = st.session_state.rag_system
    
    # Sidebar
    with st.sidebar:
        st.header("ğŸ› ï¸ Kontrollet e Sistemit")
        
        # Document statistics
        total_docs = len(rag_system.legal_documents)
        hardcoded_count = len([d for d in rag_system.legal_documents if d.get('document_type') == 'hardcoded'])
        scraped_count = total_docs - hardcoded_count
        
        st.metric("ğŸ“š Total Dokumente", total_docs)
        st.metric("ğŸ“– Dokumente BazÃ«", hardcoded_count)
        st.metric("ğŸŒ Dokumente tÃ« Shkarkuara", scraped_count)
        
        # Cloud LLM status
        st.divider()
        st.subheader("ğŸŒ Cloud AI Status")
        if rag_system.cloud_llm and rag_system.cloud_llm.api_key:
            st.success("âœ… Cloud AI Connected")
            st.info(f"ğŸ¤– Model: {rag_system.cloud_llm.model}")
        else:
            st.warning("âš ï¸ Using Fallback Responses")
        
        st.divider()
        
        # Scraping controls
        st.subheader("ğŸ”„ PÃ«rditÃ«sim Dokumentesh")
        
        if st.button("ğŸŒ Shkarko Dokumente tÃ« Reja", type="primary"):
            rag_system.scrape_new_documents()
        
        st.caption("Shkarkon dokumente tÃ« reja nga qbz.gov.al")
        
        st.divider()
        
        # Help section
        st.subheader("â“ Si tÃ« PÃ«rdorni")
        st.write("""
        1. **Shkruani pyetjen tuaj** nÃ« shqip ose anglisht
        2. **Shtypni Enter** ose klikoni KÃ«rko
        3. **Lexoni pÃ«rgjigjen** e bazuar nÃ« ligjet shqiptare
        4. **PÃ«rdorni "Shkarko Dokumente"** pÃ«r pÃ«rditÃ«sime
        
        **ğŸŒ E re**: PÃ«rgjigjet tani gjenerohen nga AI i avancuar!
        """)
        
        # Examples
        st.subheader("ğŸ’¡ Shembuj Pyetjesh")
        example_queries = [
            "Sa dit pushimi kam nÃ« punÃ« tÃ« detyrueshme?",
            "Si themeloj njÃ« biznes nÃ« ShqipÃ«ri?",
            "Cilat janÃ« tÃ« drejtat e fÃ«mijÃ«ve?",
            "Si funksionon procedura e divorcit?",
            "Ã‡farÃ« taksash duhet tÃ« paguaj?"
        ]
        
        for query in example_queries:
            if st.button(f"ğŸ’¬ {query}", key=f"example_{hash(query)}"):
                st.session_state.example_query = query
    
    # Main content area
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ’¬ BÃ«ni Pyetjen Tuaj")
        
        # Query input
        default_query = st.session_state.get('example_query', '')
        query = st.text_area(
            "Shkruani pyetjen tuaj ligjore:",
            value=default_query,
            placeholder="P.sh. Sa dit pushimi kam nÃ« punÃ« tÃ« detyrueshme?",
            height=100
        )
        
        # Clear example query after use
        if 'example_query' in st.session_state:
            del st.session_state.example_query
        
        # Search button
        if st.button("ğŸ” KÃ«rko PÃ«rgjigje", type="primary") or query:
            if query.strip():
                with st.spinner("ğŸ” Duke kÃ«rkuar nÃ« dokumentet ligjore..."):
                    # Search for relevant documents
                    relevant_docs = rag_system.search_documents(query, top_k=3)
                    
                    # Generate response
                    with st.spinner("ğŸ¤– Duke gjeneruar pÃ«rgjigje me AI..."):
                        response = rag_system.generate_response(query, relevant_docs)
                    
                    # Display response
                    st.subheader("ğŸ“‹ PÃ«rgjigja")
                    st.markdown(response)
                    
                    # Show source documents
                    if relevant_docs:
                        with st.expander("ğŸ“š Dokumentet e PÃ«rdorura", expanded=False):
                            for i, doc in enumerate(relevant_docs, 1):
                                # Determine document type with safe access
                                doc_type = "ğŸŒ Dokument i shkarkuar" if doc.get('document_type') == 'scraped_legal_document' else \
                                          "ï¿½ PDF Document" if doc.get('document_type') == 'local_pdf' else \
                                          "ï¿½ğŸ“š Dokument bazÃ«"
                                
                                st.write(f"**{i}. {doc.get('title', 'Unknown Title')}** ({doc_type})")
                                st.write(f"Relevanca: {doc.get('similarity', 0):.2%}")
                                
                                # Safe access to source field
                                source = doc.get('source', doc.get('filename', doc.get('source_url', 'Unknown Source')))
                                st.write(f"Burimi: {source}")
                                
                                if doc.get('url'):
                                    st.write(f"URL: {doc['url']}")
                                st.write("---")
                    else:
                        # No relevant documents found
                        st.info("â„¹ï¸ **AsnjÃ« dokument specifik nuk u gjet pÃ«r kÃ«tÃ« pyetje**")
                        st.write("PÃ«rgjigja e mÃ«sipÃ«rme Ã«shtÃ« bazuar nÃ« njohuritÃ« e pÃ«rgjithshme pÃ«r tÃ« drejtÃ«n shqiptare.")
                        st.write("ğŸ’¡ **Sugjerim**: Kontaktoni njÃ« jurist tÃ« kualifikuar pÃ«r kÃ«shilla tÃ« detajuara ligjore.")
            else:
                st.warning("âš ï¸ Ju lutem shkruani njÃ« pyetje")
    
    with col2:
        st.header("ğŸ“Š Statistika Sistemi")
        
        # System status
        model_status = "âœ… I ngarkuar" if rag_system.model else "âŒ Gabim"
        embeddings_status = "âœ… Gati" if rag_system.document_embeddings is not None else "âŒ MungojnÃ«"
        cloud_status = "âœ… I lidhur" if rag_system.cloud_llm and rag_system.cloud_llm.api_key else "âš ï¸ Fallback"
        
        st.metric("ğŸ¤– Modeli AI", model_status)
        st.metric("ğŸ”¢ Embeddings", embeddings_status)
        st.metric("ğŸŒ Cloud AI", cloud_status)
        
        if rag_system.document_embeddings is not None:
            embedding_shape = rag_system.document_embeddings.shape
            st.caption(f"Dimensioni: {embedding_shape[0]}Ã—{embedding_shape[1]}")
        
        # Last update info
        if os.path.exists(rag_system.scraper.metadata_file):
            try:
                with open(rag_system.scraper.metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    if metadata.get('last_updated'):
                        last_update = datetime.fromisoformat(metadata['last_updated'])
                        st.metric("ğŸ•’ PÃ«rditÃ«simi i Fundit", last_update.strftime("%d/%m/%Y %H:%M"))
            except:
                pass
        
        # Performance metrics
        st.subheader("âš¡ Performance")
        st.metric("ğŸ“ˆ Dokumente Aktive", len(rag_system.legal_documents))
        
        # Quick stats about document types
        doc_types = {}
        for doc in rag_system.legal_documents:
            doc_type = doc.get('document_type', 'unknown')
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        
        for doc_type, count in doc_types.items():
            if doc_type == 'hardcoded':
                st.metric("ğŸ“– BazÃ«", count)
            elif doc_type == 'scraped_legal_document':
                st.metric("ğŸŒ TÃ« shkarkuara", count)
    
    # Footer
    st.divider()
    st.caption("âš–ï¸ Albanian Legal RAG System - Now powered by Advanced Cloud AI (Groq)")
    st.caption("ğŸ”„ Automatically updates with latest documents from qbz.gov.al")


if __name__ == "__main__":
    main()
