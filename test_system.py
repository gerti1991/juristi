#!/usr/bin/env python3
"""
Albanian Legal RAG System - Comprehensive Test Suite
Tests all major components and functionality
"""

import os
import sys
import json
import traceback
from datetime import datetime

def test_basic_imports():
    """Test basic Python imports"""
    print("ğŸ” Testing basic imports...")
    try:
        import json
        import os
        import sys
        print("   âœ… Standard library imports: OK")
        return True
    except Exception as e:
        print(f"   âŒ Basic imports failed: {e}")
        return False

def test_dependencies():
    """Test external dependencies"""
    print("ğŸ” Testing external dependencies...")
    dependencies = [
        ('streamlit', 'Streamlit web framework'),
        ('groq', 'Groq API client'),
        ('sentence_transformers', 'Sentence Transformers'),
        ('numpy', 'NumPy'),
        ('sklearn', 'Scikit-learn'),
        ('requests', 'HTTP requests'),
        ('dotenv', 'Environment variables')
    ]
    
    success_count = 0
    for package, description in dependencies:
        try:
            __import__(package)
            print(f"   âœ… {package}: OK")
            success_count += 1
        except ImportError as e:
            print(f"   âŒ {package}: FAILED - {e}")
    
    print(f"   ğŸ“Š Dependencies: {success_count}/{len(dependencies)} working")
    return success_count == len(dependencies)

def test_project_structure():
    """Test project file structure"""
    print("ğŸ” Testing project structure...")
    
    required_files = [
        'app.py',
        'ai.py', 
        'scraper.py',
        'config.py',
        'requirements.txt',
        '.env',
        'README.md'
    ]
    
    required_dirs = [
        'legal_documents',
        'legal_documents/pdfs',
        'legal_documents/processed'
    ]
    
    success_count = 0
    
    # Check files
    for file in required_files:
        if os.path.exists(file):
            print(f"   âœ… {file}: EXISTS")
            success_count += 1
        else:
            print(f"   âŒ {file}: MISSING")
    
    # Check directories  
    for directory in required_dirs:
        if os.path.exists(directory):
            print(f"   âœ… {directory}/: EXISTS")
            success_count += 1
        else:
            print(f"   âŒ {directory}/: MISSING")
    
    total_items = len(required_files) + len(required_dirs)
    print(f"   ğŸ“Š Project structure: {success_count}/{total_items} items found")
    return success_count >= total_items * 0.8  # 80% threshold

def test_legal_documents():
    """Test legal document loading"""
    print("ğŸ” Testing legal documents...")
    
    try:
        pdf_file = os.path.join("legal_documents", "pdf_rag_documents.json")
        
        if not os.path.exists(pdf_file):
            print("   âŒ PDF documents file not found")
            return False
            
        with open(pdf_file, 'r', encoding='utf-8') as f:
            docs = json.load(f)
        
        if len(docs) == 0:
            print("   âŒ No documents loaded")
            return False
            
        print(f"   âœ… {len(docs)} document chunks loaded")
        
        # Count document types
        doc_types = {}
        for doc in docs:
            title = doc.get('title', 'Unknown')
            doc_types[title] = doc_types.get(title, 0) + 1
        
        print(f"   âœ… {len(doc_types)} different legal codes found")
        
        # Check key documents
        key_docs = [
            "Kodi Civil 2023",
            "Kodi I Punes Ligj 2024",
            "Ligj Nr. 7895 1995 Kodi Penal I PÃ«rditÃ«suar"
        ]
        
        found_key_docs = 0
        for key_doc in key_docs:
            if any(key_doc in title for title in doc_types.keys()):
                found_key_docs += 1
                
        print(f"   âœ… {found_key_docs}/{len(key_docs)} key legal codes found")
        return True
        
    except Exception as e:
        print(f"   âŒ Legal documents test failed: {e}")
        return False

def test_environment_config():
    """Test environment configuration"""
    print("ğŸ” Testing environment configuration...")
    
    try:
        from dotenv import load_dotenv
        load_dotenv()
        
        # Check API key
        api_key = os.getenv('GROQ_API_KEY')
        if api_key and len(api_key) > 10:
            print("   âœ… Groq API key configured")
        else:
            print("   âš ï¸  Groq API key not found or invalid")
            
        # Check other config
        model = os.getenv('GROQ_MODEL', 'not set')
        print(f"   âœ… Groq model: {model}")
        
        threshold = os.getenv('SIMILARITY_THRESHOLD', 'not set')
        print(f"   âœ… Similarity threshold: {threshold}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Environment config test failed: {e}")
        return False

def test_core_modules():
    """Test core application modules"""
    print("ğŸ” Testing core modules...")
    
    modules_to_test = [
        ('ai', 'CloudLLMClient'),
        ('scraper', 'AlbanianLegalScraper'),
        ('config', None)
    ]
    
    success_count = 0
    
    for module_name, class_name in modules_to_test:
        try:
            module = __import__(module_name)
            if class_name:
                getattr(module, class_name)
                print(f"   âœ… {module_name}.{class_name}: OK")
            else:
                print(f"   âœ… {module_name}: OK")
            success_count += 1
        except Exception as e:
            print(f"   âŒ {module_name}: FAILED - {e}")
    
    print(f"   ğŸ“Š Core modules: {success_count}/{len(modules_to_test)} working")
    return success_count == len(modules_to_test)

def test_rag_functionality():
    """Test RAG system functionality"""
    print("ğŸ” Testing RAG functionality...")
    
    try:
        # Test sentence transformer
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Test embedding
        test_text = "Sa dit pushimi kam nÃ« punÃ«?"
        embedding = model.encode([test_text])
        
        if embedding is not None and len(embedding) > 0:
            print("   âœ… Sentence embedding: OK")
        else:
            print("   âŒ Sentence embedding: FAILED")
            return False
            
        # Test similarity calculation
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity
        
        embedding1 = model.encode(["test text 1"])
        embedding2 = model.encode(["test text 2"])
        similarity = cosine_similarity(embedding1, embedding2)[0][0]
        
        print(f"   âœ… Similarity calculation: OK (score: {similarity:.3f})")
        return True
        
    except Exception as e:
        print(f"   âŒ RAG functionality test failed: {e}")
        traceback.print_exc()
        return False

def run_all_tests():
    """Run all tests and provide summary"""
    print("ğŸ§ª ALBANIAN LEGAL RAG SYSTEM - FULL TEST SUITE")
    print("=" * 60)
    print(f"ğŸ•’ Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    tests = [
        ("Basic Imports", test_basic_imports),
        ("Dependencies", test_dependencies),
        ("Project Structure", test_project_structure),
        ("Legal Documents", test_legal_documents),
        ("Environment Config", test_environment_config),
        ("Core Modules", test_core_modules),
        ("RAG Functionality", test_rag_functionality)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª Running {test_name} test...")
        try:
            result = test_func()
            results.append((test_name, result))
            if result:
                print(f"   ğŸ‰ {test_name}: PASSED")
            else:
                print(f"   ğŸ’¥ {test_name}: FAILED")
        except Exception as e:
            print(f"   ğŸ’¥ {test_name}: ERROR - {e}")
            results.append((test_name, False))
        print()
    
    # Summary
    print("ğŸ“Š TEST SUMMARY")
    print("-" * 30)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   {status} {test_name}")
    
    print()
    print(f"ğŸ“ˆ Overall Result: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ ALL TESTS PASSED! Your Albanian Legal RAG system is ready!")
        print("ğŸš€ You can now run: streamlit run app.py")
    elif passed >= total * 0.8:
        print("âš ï¸  Most tests passed. System should work with minor issues.")
    else:
        print("âŒ Several tests failed. Please check the issues above.")
    
    print(f"ğŸ•’ Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    return passed == total

if __name__ == "__main__":
    run_all_tests()
